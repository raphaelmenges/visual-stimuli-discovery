{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Takes results of one example-based evaluation through Evaluator.exe and Preciser.exe and computes precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'walmart' # 'webmd', 'walmart', 'guardian', 'cnn'\n",
    "participants = ['P1', 'P2', 'P3', 'P4']\n",
    "evaluation = 'gt-walmart'\n",
    "dataset_evaluation_dir = 'C:/GazeMiningDataset/Dataset_evaluation/case-study'\n",
    "dataset_stimuli_dir = r'C:/GazeMiningDataset/Dataset_stimuli'\n",
    "\n",
    "# Build up pathes\n",
    "labels_screencasts_filepath = dataset_evaluation_dir + '/' + evaluation + '-screencasts.csv'\n",
    "labels_stimuli_filepath = dataset_evaluation_dir + '/' + evaluation + '-stimuli.csv'\n",
    "contrib_filepath = dataset_evaluation_dir + '/' + evaluation + '-contrib.csv'\n",
    "events_filepath = dataset_evaluation_dir + '/' + evaluation + '-events.csv'\n",
    "stimuli_dir = dataset_stimuli_dir + '/' + site + '/stimuli'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluator labels of stimuli\n",
    "stimuli_df = pd.read_csv(labels_stimuli_filepath)\n",
    "\n",
    "# Load evaluator labels of screencasts\n",
    "screencasts_df = pd.read_csv(labels_screencasts_filepath)\n",
    "\n",
    "# Load contribution (this tells one which frames as contained in the stimuli contributed to the element in the task)\n",
    "contrib_df = pd.read_csv(contrib_filepath)\n",
    "\n",
    "# Load events\n",
    "events_df = pd.read_csv(events_filepath, header=None, names=['timestamp', 'type', 'event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process events for general information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Mode [s]: 356.671, Stimuli Mode [s]: 209.147\n"
     ]
    }
   ],
   "source": [
    "mode_changes_df = events_df[(events_df.type == 'mode_change')]\n",
    "\n",
    "# Get timestamps\n",
    "start_ts = int(mode_changes_df[(mode_changes_df.event == 'mode_start')].timestamp)\n",
    "videos_ts = int(mode_changes_df[(mode_changes_df.event == 'mode_videos')].timestamp)\n",
    "stimuli_ts = int(mode_changes_df[(mode_changes_df.event == 'mode_stimuli')].timestamp)\n",
    "end_ts = int(mode_changes_df[(mode_changes_df.event == 'mode_end')].timestamp)\n",
    "\n",
    "# Compute durations of modes\n",
    "video_duration = 0\n",
    "stimuli_duration = 0\n",
    "if videos_ts > stimuli_ts:\n",
    "    video_duration = end_ts - videos_ts\n",
    "    stimuli_duration = videos_ts - stimuli_ts\n",
    "else:\n",
    "    video_duration = stimuli_ts - videos_ts\n",
    "    stimuli_duration = end_ts - stimuli_ts\n",
    "    \n",
    "# Print results\n",
    "print('Video Mode [s]: ' + str(video_duration/1000) + ', Stimuli Mode [s]: ' + str(stimuli_duration/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect frames represented by marked stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make groups by layer id from labeled stimuli\n",
    "layers = stimuli_df.groupby(['layer_id'])\n",
    "\n",
    "# Set of frames that are represented by the stimuli\n",
    "marked_stimuli_frames = defaultdict(set) # participant_id -> set of frames\n",
    "\n",
    "# Go over layer groups\n",
    "for layer_id, data in layers:\n",
    "    \n",
    "    # Get ids of stimuli that are marked\n",
    "    marked_stimuli_ids = data[data.label == 1]['stimulus_id']\n",
    "    \n",
    "    # Go over marked stimuli and collect all represented frames across shots per screencast\n",
    "    for stimulus_id in marked_stimuli_ids:\n",
    "        df = pd.read_csv(stimuli_dir + '/' + layer_id + '/' + str(stimulus_id) +'-shots.csv') # read in information about stimulus (which frames are contained...)\n",
    "        for index, row in df.iterrows(): # go over contained shots and collect the frames\n",
    "            frames = list(range(row['frame_idx_start'], row['frame_idx_end']+1, 1))\n",
    "            participant_id = row['session_id'][:2].upper()\n",
    "            marked_stimuli_frames[participant_id].update(frames) # put frames into the set, one set per screencast\n",
    "            \n",
    "# print(marked_stimuli_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrib Frame Count: 859\n"
     ]
    }
   ],
   "source": [
    "# There are some duplicated frames in the contrib files, fix that\n",
    "# Note: This happens, when one frame is separated for stimuli discovery\n",
    "# into more than one layer and the element is found on both layers in the evaluation\n",
    "groups = contrib_df.groupby(['session'])\n",
    "new_df = pd.DataFrame()\n",
    "for key in groups.groups.keys():\n",
    "    df = groups.get_group(key)\n",
    "    df = df.drop_duplicates(subset='frame_idx')\n",
    "    new_df = new_df.append(df, ignore_index=True)\n",
    "contrib_df = new_df\n",
    "\n",
    "print('Contrib Frame Count: ' + str(contrib_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_CONTRIB: 464\n",
      "NEG_CONTRIB: 0\n",
      "NEUTRAL: 395\n"
     ]
    }
   ],
   "source": [
    "pos_contrib_count = contrib_df[contrib_df.label == 'POS_CONTRIB'].shape[0]\n",
    "neg_contrib_count = contrib_df[contrib_df.label == 'NEG_CONTRIB'].shape[0]\n",
    "neutral_count = contrib_df[contrib_df.label == 'NEUTRAL'].shape[0]\n",
    "\n",
    "print('POS_CONTRIB: ' + str(pos_contrib_count))\n",
    "print('NEG_CONTRIB: ' + str(neg_contrib_count))\n",
    "print('NEUTRAL: ' + str(neutral_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect frames marked in screencasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P1': Int64Index([132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
      "            145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 188,\n",
      "            189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
      "            202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 217, 218],\n",
      "           dtype='int64'), 'P2': Int64Index([ 844,  845,  846,  847,  848,  849,  850,  851,  852,  853,\n",
      "            ...\n",
      "            1027, 1028, 1029, 1030, 1031, 1032, 1033, 1039, 1040, 1041],\n",
      "           dtype='int64', length=191), 'P3': Int64Index([194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
      "            207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "            362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374,\n",
      "            375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387,\n",
      "            388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n",
      "            401, 404, 405, 406, 629, 630, 851, 852, 853, 854, 855, 856, 857,\n",
      "            858, 859, 860, 861, 862, 863, 864, 865],\n",
      "           dtype='int64'), 'P4': Int64Index([ 500,  501,  502,  503,  504,  505,  506,  507,  508,  509,\n",
      "            ...\n",
      "            1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066],\n",
      "           dtype='int64', length=147)}\n",
      "Annotated Frames: 4524\n",
      "Display AOI: 476\n",
      "Do not display AOI: 4048\n"
     ]
    }
   ],
   "source": [
    "# Marked frames of screencasts\n",
    "marked_screencast_frames = {}\n",
    "marked_screencast_frames['P1'] = screencasts_df.index[screencasts_df.P1 == 1]\n",
    "marked_screencast_frames['P2'] = screencasts_df.index[screencasts_df.P2 == 1]\n",
    "marked_screencast_frames['P3'] = screencasts_df.index[screencasts_df.P3 == 1]\n",
    "marked_screencast_frames['P4'] = screencasts_df.index[screencasts_df.P4 == 1]\n",
    "\n",
    "print(marked_screencast_frames)\n",
    "\n",
    "# Count frames\n",
    "p1_no = len(screencasts_df.index[screencasts_df.P1 == 0])\n",
    "p1_yes = len(screencasts_df.index[screencasts_df.P1 == 1])\n",
    "p2_no = len(screencasts_df.index[screencasts_df.P2 == 0])\n",
    "p2_yes = len(screencasts_df.index[screencasts_df.P2 == 1])\n",
    "p3_no = len(screencasts_df.index[screencasts_df.P3 == 0])\n",
    "p3_yes = len(screencasts_df.index[screencasts_df.P3 == 1])\n",
    "p4_no = len(screencasts_df.index[screencasts_df.P4 == 0])\n",
    "p4_yes = len(screencasts_df.index[screencasts_df.P4 == 1])\n",
    "\n",
    "print('Annotated Frames: ' + str(p1_no + p1_yes + p2_no + p2_yes + p3_no + p3_yes + p4_no + p4_yes))\n",
    "print('Display AOI: ' + str(p1_yes + p2_yes + p3_yes + p4_yes))\n",
    "print('Do not display AOI: ' + str(p1_no + p2_no + p3_no + p4_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimuli Frames Count: 708\n",
      "Screencast Frames Count: 316\n",
      "Precision: 0.9269662921348315\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare both dictionaries\n",
    "stimuli_frames_count = 0;\n",
    "recall_count = 0\n",
    "screencast_frames_count = 0\n",
    "for participant in participants:\n",
    "    \n",
    "    # Get frames marked (indirectly) in stimuli and screencasts\n",
    "    stimuli_frames = set(marked_stimuli_frames[participant])\n",
    "    screencast_frames = set(marked_screencast_frames[participant])\n",
    "    stimuli_frames_count += len(stimuli_frames)\n",
    "    screencast_frames_count += len(screencast_frames)\n",
    "    \n",
    "    # Compute recall\n",
    "    recall_count += len(screencast_frames.intersection(stimuli_frames)) # frames that are contained in both sets\n",
    "\n",
    "print('Stimuli Frames Count: ' + str(stimuli_frames_count))\n",
    "print('Screencast Frames Count: ' + str(screencast_frames_count))\n",
    "print('Precision: ' + str(pos_contrib_count / (pos_contrib_count + neg_contrib_count)))\n",
    "print('Recall: ' + str(recall_count / screencast_frames_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
