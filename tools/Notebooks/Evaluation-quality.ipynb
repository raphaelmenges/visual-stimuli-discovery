{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This script checks blind pixels (aka animations), evaluates the finder output and counts pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stimuli_dir = r'C:/GazeMiningDataset/Dataset_stimuli'\n",
    "dataset_visual_change_dir = r'C:/GazeMiningDataset/Dataset_visual_change'\n",
    "dataset_overall_evaluation_dir = r'C:/GazeMiningDataset/Dataset_evaluation/quality'\n",
    "participants = ['p1', 'p2', 'p3', 'p4']\n",
    "screencast_width = 1024\n",
    "screencast_height = 768\n",
    "\n",
    "# Categories\n",
    "shopping = ['walmart', 'amazon', 'steam']\n",
    "news = ['reddit', 'cnn', 'guardian']\n",
    "health = ['nih', 'webmd', 'mayo']\n",
    "cars = ['gm', 'nissan', 'kia']\n",
    "categories = {'shopping': shopping, 'news': news, 'health': health, 'cars': cars}\n",
    "\n",
    "# Folder with stimuli from root layer\n",
    "root_layer_stimuli_dir = {\n",
    "    'amazon': dataset_stimuli_dir + '/amazon/stimuli/0_html',\n",
    "    'steam': dataset_stimuli_dir + '/steam/stimuli/0_html',\n",
    "    'walmart': dataset_stimuli_dir + '/walmart/stimuli/1_html',\n",
    "    \n",
    "    'cnn': dataset_stimuli_dir + '/cnn/stimuli/0_html',\n",
    "    'guardian': dataset_stimuli_dir + '/guardian/stimuli/1_html',\n",
    "    'reddit': dataset_stimuli_dir + '/reddit/stimuli/0_html',\n",
    "    \n",
    "    'mayo': dataset_stimuli_dir + '/mayo/stimuli/0_html',\n",
    "    'nih': dataset_stimuli_dir + '/nih/stimuli/1_html',\n",
    "    'webmd': dataset_stimuli_dir + '/webmd/stimuli/1_html',\n",
    "    \n",
    "    'nissan': dataset_stimuli_dir + '/nissan/stimuli/0_html',\n",
    "    'gm': dataset_stimuli_dir + '/gm/stimuli/2_html',\n",
    "    'kia': dataset_stimuli_dir + '/kia/stimuli/0_html'\n",
    "}\n",
    "\n",
    "# Regex\n",
    "png_regex = re.compile('.png$') # to filter for .png files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go over categories\n",
    "for (_,sites) in categories.items():\n",
    "    \n",
    "    # Go over sites\n",
    "    for site in sites:\n",
    "\n",
    "        # Stimuli from root layer\n",
    "        stimuli_files_root = [f for f in os.listdir(root_layer_stimuli_dir[site]) if os.path.isfile(os.path.join(root_layer_stimuli_dir[site],f))]\n",
    "        stimuli_files_root = list(filter(png_regex.search, stimuli_files_root))\n",
    "        stimuli_count_root = len(stimuli_files_root)\n",
    "        \n",
    "        # Stimuli files across all layers\n",
    "        stimuli_count = 0\n",
    "        stimuli_dir = os.path.join(dataset_stimuli_dir, site, 'stimuli')\n",
    "        layers = [d for d in os.listdir(stimuli_dir) if os.path.isdir(os.path.join(stimuli_dir,d))]\n",
    "        for layer in layers: # go over layers\n",
    "            \n",
    "            # Find stimuli\n",
    "            layer_dir = os.path.join(stimuli_dir, layer)\n",
    "            stimuli_files = [f for f in os.listdir(layer_dir) if os.path.isfile(os.path.join(layer_dir,f))]\n",
    "            stimuli_files = list(filter(png_regex.search, stimuli_files))\n",
    "            stimuli_count += len(stimuli_files)\n",
    "        \n",
    "        # Frame counts in screencasts\n",
    "        frame_count = 0\n",
    "        for p in participants:\n",
    "            df = pd.read_csv(dataset_visual_change_dir + '/' + p + '/' + site + '_meta.csv')\n",
    "            frame_count += df.iloc[0,2] # screencast_frame_total_count\n",
    "        \n",
    "        # Print information\n",
    "        print(site)\n",
    "        print('Stimuli count (root layer): ' + str(stimuli_count_root))\n",
    "        print('Stimuli count: ' + str(stimuli_count))\n",
    "        print('Frame count: ' + str(frame_count))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blind frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Blind frames of root layer')\n",
    "\n",
    "# Prepare variables\n",
    "shots_regex = re.compile('-shots.csv$') # to filter for -shots.csv files\n",
    "blind_frame_idxs = defaultdict(list)\n",
    "blind_frame_freq = defaultdict(int) # overall\n",
    "blind_frame_front_freq = defaultdict(int)\n",
    "blind_frame_back_freq = defaultdict(int)\n",
    "\n",
    "# Go over categories\n",
    "for (_,sites) in categories.items():\n",
    "    \n",
    "    # Go over sites\n",
    "    for site in sites:\n",
    "        \n",
    "        # Retrieve shots file per stimulus of root layer\n",
    "        shots_files = [f for f in os.listdir(root_layer_stimuli_dir[site]) if os.path.isfile(os.path.join(root_layer_stimuli_dir[site],f))]\n",
    "        shots_files = list(filter(shots_regex.search, shots_files))\n",
    "\n",
    "        site_count = 0 # count of blind frames per site\n",
    "        site_front_count = 0\n",
    "        site_back_count = 0\n",
    "\n",
    "        # For each shot file, get blind files\n",
    "        for shots_file in shots_files:\n",
    "            df_shots = pd.read_csv(root_layer_stimuli_dir[site] + \"/\" + shots_file)\n",
    "            \n",
    "            for session, shot_idx, frame_idx_start, frame_idx_end in zip(df_shots.session_id, df_shots.shot_idx, df_shots.frame_idx_start, df_shots.frame_idx_end): \n",
    "                \n",
    "                # Load data\n",
    "                shots_dir = os.path.join(dataset_stimuli_dir, site, 'shots')\n",
    "                df_blind = pd.read_csv(shots_dir + '/' + session + '_' + str(shot_idx) + '-blind.csv')\n",
    "                \n",
    "                # Check whether blind frames are added in the front or back\n",
    "                front_blind = []\n",
    "                back_blind = []\n",
    "                non_blind_frames = list(set(range(frame_idx_start, frame_idx_end+1, 1)) - set(df_blind.frame_idx))\n",
    "                min_id = min(non_blind_frames)\n",
    "                for blind_frame_idx in df_blind.frame_idx:\n",
    "                    if blind_frame_idx < min_id:\n",
    "                        front_blind.append(blind_frame_idx)\n",
    "                    else:\n",
    "                        back_blind.append(blind_frame_idx)\n",
    "                \n",
    "                # Compute counts\n",
    "                count = len(df_blind.frame_idx) # count of blind frames\n",
    "                blind_frame_idxs[site] += list(df_blind.frame_idx) # blind frame idxs per site\n",
    "                blind_frame_freq[count] += 1 # frequency of blind frame count\n",
    "                blind_frame_front_freq[len(front_blind)] += 1 # frequency of blind frame count at front\n",
    "                blind_frame_back_freq[len(back_blind)] += 1 # frequency of blind frame count at back\n",
    "                site_count += count # count of blind frames per site\n",
    "                site_front_count += len(front_blind)\n",
    "                site_back_count += len(back_blind)\n",
    "\n",
    "        print(site + ': ' + str(site_count) + ' Front: ' + str(site_front_count) + ' Back: ' + str(site_back_count))\n",
    "\n",
    "# Sort dictionaries before printing\n",
    "blind_frame_freq = OrderedDict(sorted(blind_frame_freq.items()))\n",
    "blind_frame_front_freq = OrderedDict(sorted(blind_frame_front_freq.items()))\n",
    "blind_frame_back_freq = OrderedDict(sorted(blind_frame_back_freq.items()))\n",
    "\n",
    "# Print dictionaries\n",
    "print()\n",
    "print('Blind frame frequencies (root layer):')\n",
    "\n",
    "print(blind_frame_freq)\n",
    "print()\n",
    "\n",
    "print('Blind frame frequencies in front (root layer):')\n",
    "print(blind_frame_front_freq)\n",
    "print()\n",
    "\n",
    "print('Blind frame frequencies in back (root layer):')\n",
    "print(blind_frame_back_freq)\n",
    "print()\n",
    "\n",
    "# Calculate averages\n",
    "'''\n",
    "agg = 0\n",
    "n = 0\n",
    "for blind_count, freq in blind_frame_freq.items():\n",
    "    agg += blind_count * freq\n",
    "    n += freq\n",
    "print('Blind frames mean: ' + str(agg / n))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall (Finder output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_regex = re.compile('.csv$') # to filter for .csv files\n",
    "\n",
    "# Get output files of Finder.exe\n",
    "finder_files = [f for f in os.listdir(dataset_overall_evaluation_dir) if os.path.isfile(os.path.join(dataset_overall_evaluation_dir,f))]\n",
    "finder_files = list(filter(csv_regex.search, finder_files))\n",
    "\n",
    "# Analyze each file\n",
    "for finder_file in finder_files:\n",
    "    \n",
    "    site = finder_file.replace('_finder.csv','')\n",
    "    print('### ' + site)\n",
    "    df = pd.read_csv(dataset_overall_evaluation_dir + \"/\" + finder_file)\n",
    "    \n",
    "    # Counts\n",
    "    same_count = np.count_nonzero(df.same)\n",
    "    pixel_perfect_count = np.count_nonzero(df.pixel_perfect_same)\n",
    "    frame_count = len(df.same)\n",
    "    wrong_frames = df.loc[df['same'] == 0].frame_idx\n",
    "    intersection_blind_and_wrong = list(set(blind_frame_idxs[site]) & set(wrong_frames))\n",
    "    really_wrong_count = len(wrong_frames) - len(intersection_blind_and_wrong)\n",
    "    \n",
    "    # Inform user\n",
    "    print('Correct Ratio (root layer): ' + str(same_count/frame_count))\n",
    "    print('Correct Ratio (only really wrong, root layer): ' + str((frame_count - really_wrong_count)/frame_count))\n",
    "    print('Pixel perfect count, root layer: ' + str(pixel_perfect_count))  \n",
    "    print(\n",
    "        'Frame count (root layer): '\n",
    "        + str(frame_count) +\n",
    "        ' Blind count: '\n",
    "        + str(len(blind_frame_idxs[site])) +\n",
    "        ' Wrong frames: '\n",
    "        + str(len(wrong_frames)) +\n",
    "        ' Intersection with blind frames: '\n",
    "        + str(len(intersection_blind_and_wrong)) +\n",
    "        ' Really wrong: '\n",
    "        + str(really_wrong_count))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to compute\n",
    "overall_stimuli_pixels = 0\n",
    "overall_screencast_pixels = 0\n",
    "\n",
    "# Go over categories\n",
    "for (_,sites) in categories.items():\n",
    "    \n",
    "    # Go over sites\n",
    "    for site in sites:\n",
    "        \n",
    "        print('# ' + site)\n",
    "        print()\n",
    "        \n",
    "        # Values to compute\n",
    "        site_stimuli_pixels = 0\n",
    "        site_screencast_pixels = 0\n",
    "        \n",
    "        # First, compute opaque pixels in the discovered stimuli\n",
    "        stimuli_dir = os.path.join(dataset_stimuli_dir, site, 'stimuli')\n",
    "        layers = [d for d in os.listdir(stimuli_dir) if os.path.isdir(os.path.join(stimuli_dir,d))]\n",
    "        for layer in layers: # go over layers\n",
    "            \n",
    "            # Use this to restrict pixel ratio to root-layer stimuli, only\n",
    "            if not layer in root_layer_stimuli_dir[site]:\n",
    "                continue\n",
    "            \n",
    "            print('--> ' + layer)\n",
    "            layer_pixel_count = 0\n",
    "            \n",
    "            # Find stimuli\n",
    "            layer_dir = os.path.join(stimuli_dir, layer)\n",
    "            stimuli_files = [f for f in os.listdir(layer_dir) if os.path.isfile(os.path.join(layer_dir,f))]\n",
    "            stimuli_files = list(filter(png_regex.search, stimuli_files))\n",
    "            # print('Found ' + str(len(stimuli_files)) + ' simuli')\n",
    "            \n",
    "            # Go over stimuli and count pixels\n",
    "            for stimulus_file in stimuli_files:\n",
    "                img = Image.open(layer_dir + '/' + stimulus_file)\n",
    "                img = img.convert('RGBA')\n",
    "                pixdata = img.load()\n",
    "                width, height = img.size\n",
    "                for y in range(height):\n",
    "                    for x in range(width):\n",
    "                        if pixdata[x, y][3] > 0:\n",
    "                            layer_pixel_count += 1\n",
    "                            \n",
    "            # Accumulate pixel count of site\n",
    "            site_stimuli_pixels += layer_pixel_count\n",
    "                            \n",
    "            # Print pixel count\n",
    "            # print('Found ' + str(layer_pixel_count) + ' opaque pixels for the layer')\n",
    "            \n",
    "        # Second, compute pixels in screencasts\n",
    "        for p in participants:\n",
    "            df = pd.read_csv(dataset_visual_change_dir + '/' + p + '/' + site + '_meta.csv')\n",
    "            frame_count = df.iloc[0,2] # screencast_frame_total_count\n",
    "            site_screencast_pixels += screencast_width * screencast_height * frame_count\n",
    "            \n",
    "        # Accumulate overall\n",
    "        overall_stimuli_pixels += site_stimuli_pixels\n",
    "        overall_screencast_pixels += site_screencast_pixels\n",
    "\n",
    "        # Calculate percentage\n",
    "        site_percentage = 100.0 * (float(site_stimuli_pixels) / float(site_screencast_pixels))\n",
    "        print('Stimuli Pixels in relation to Screencast Pixels = ' + str(site_percentage) + '%')\n",
    "        print()\n",
    "        \n",
    "# Calculate percentage\n",
    "overall_percentage = 100.0 * (float(overall_stimuli_pixels) / float(overall_screencast_pixels))\n",
    "print('Overall: Stimuli Pixels in relation to Screencast Pixels = ' + str(overall_percentage) + '%')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
